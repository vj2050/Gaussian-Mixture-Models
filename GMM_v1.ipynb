{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOMEWORK 5\n",
    "#### VAISHNAVI JAMDADE(TM39453)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#from random import normal\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score,r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating gaussian distributions and then clustering them together as one complete input dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdata(v):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    dis1=np.random.normal(0,v,50)\n",
    "    dis2=np.random.normal(5,v,50)\n",
    "    dis3=np.random.normal(10,v,50)\n",
    "    A=np.concatenate([dis1,dis2])\n",
    "    X=np.concatenate([A,dis3])\n",
    "    X\n",
    "    return X,v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150,), 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,varT=createdata(1)\n",
    "X.shape, varT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_hist(data):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    for x in data:\n",
    "        plt.hist(x, bins = 80, normed = True, alpha = 0.7)\n",
    "    #plt.xlim(-10, 20)\n",
    "    plt.figure(figsize=(15,7))\n",
    "    \n",
    "plot_hist(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to initialize random means, variances and prior probabilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X,K):\n",
    "    \n",
    "    m=np.array([2.,3.,7.])             #random means\n",
    "    #m=np.random.choice(X,size=3) \n",
    "    #v=np.array([var1,var1,var1])\n",
    "    v=np.ones((3))                     #random variances set to 1 initially\n",
    "    w=np.ones((K))/K                   #random prior weights set to equal\n",
    "    #posterior probability that each input belongs to each of the 3 gaussians\n",
    "    posterior=np.zeros((len(X),K),dtype=float)\n",
    "    return m,v,w,posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True assignment of data samples to respective Gaussians and means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[0]*50 + [1]*50 + [2]*50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,true,K):\n",
    "    diff=[]\n",
    "    summ=0    \n",
    "    for j in range(K):\n",
    "        #d=abs(pred[j]-true[j])/true[j]\n",
    "        diff.append((abs(pred[j]-true[j])))\n",
    "    summ=sum(diff)\n",
    "    accu=round(((1-summ)*100),2)    \n",
    "    #accu=summ\n",
    "        \n",
    "    return summ,accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to allocate the probability of each of the data points belonging to each one of the clusters(Soft Allocation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM(X,K,varc,epochs):\n",
    "    N=len(X)\n",
    "    #K=3\n",
    "    pi=math.pi\n",
    "    iterations=epochs\n",
    "    M=[0.,5.,10.]\n",
    "    V=[varc,varc,varc]\n",
    "    #V=[1,1,1]\n",
    "    for iter in range(iterations):\n",
    "        \n",
    "        #Expection Step : Calculating Posterior probabilities for each of the inputs to compute probability of inputs belonging to each of the clusters:\n",
    "        for i in range(len(X)):\n",
    "            for k in range(K):\n",
    "\n",
    "                part1=1/math.sqrt(2*pi*(var[k]))\n",
    "                part2=(-1/2)*((X[i]-means[k])**2)\n",
    "                part3= part2/(var[k])\n",
    "                part4=np.exp(part3)\n",
    "                posterior[i][k]=wprior[k]*part1 * part4\n",
    "\n",
    "                #posterior[i][k]=wprior[k]*(1/math.sqrt(2*var[k]*pi))*(math.exp(-(1/2)*((X[i]-means[k])**2)))\n",
    "\n",
    "            posterior[i]=posterior[i]/np.sum(posterior[i])\n",
    "        \n",
    "        \n",
    "        #Maximization Step : Updating parameters : mean, variance, and prior probabilities :\n",
    "        meanspred=[]\n",
    "        varpred=[]\n",
    "        diffmean=[]\n",
    "        lossmean=[]\n",
    "        for i in range(K):\n",
    "            wprior[i]=np.sum(posterior[:,i])/N\n",
    "            means[i]=np.dot(posterior[:,i],X[:].T)/(np.sum(posterior[:,i]))\n",
    "            var[i]=np.dot(posterior[:,i],((X-means[i])**2).T)/(np.sum(posterior[:,i]))\n",
    "            meanspred.append(means[i])\n",
    "            varpred.append(var[i])\n",
    "        \n",
    "        ##### Retrieving Predicted class labels from posterior probabilities :\n",
    "        li=[]\n",
    "        for i in range(len(X)):\n",
    "            li.append(np.argmax(posterior[i]))\n",
    "        \n",
    "        \n",
    "        ######Accuracy Calculation for Mean and Variance at each iteration :\n",
    "        meanloss,accumean=accuracy(meanspred,M,K)\n",
    "        varloss,accuvar=accuracy(varpred,V,K)\n",
    "        ######\n",
    "        \n",
    "        \n",
    "        print(\"\\n\")    \n",
    "        print(\"Epoch \" + str(iter+1) + \" : \" + \"prior probability =\"+ str(wprior))\n",
    "        print(\"Epoch \" + str(iter+1) + \" : \" + \"mean =\"+ str(means))\n",
    "        print(\"Epoch \" + str(iter+1) + \" : \"+ \"variance =\" + str(var))\n",
    "        \n",
    "        \n",
    "        #print(meanspred)\n",
    "        print(\"Total mean loss is=> \", meanloss)\n",
    "        #print(\"R2 score for Mean => \".format(round(r2_score(meanspred,M)*100)))\n",
    "        print(\"R2 score of Means after Epoch {} : {}\".format(iter+1,round((r2_score(meanspred,M)),2)))\n",
    "        print(\"Total variance loss is=> \", varloss)\n",
    "        print(\"R2 score of Variance after Epoch {} : {}\".format(iter+1,round((r2_score(varpred,V)),2)))\n",
    "        #print(\"Accuracy of variance after Epoch {} : {}%\".format(iter+1, accuvar))\n",
    "        totalaccuracy = round((accuracy_score(li, Y)*100),2)\n",
    "        print(\"Accuracy of allocation after Epoch {} : {}%\".format(iter+1, totalaccuracy))\n",
    "        #totalaccuracy\n",
    "        \n",
    "        \n",
    "    return (wprior, means,var,totalaccuracy,li)\n",
    "    #means.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 : prior probability =[0.31686045 0.18882409 0.49431546]\n",
      "Epoch 1 : mean =[0.33717436 3.66738246 8.75161024]\n",
      "Epoch 1 : variance =[2.35235318 2.39087316 5.5254408 ]\n",
      "Total mean loss is=>  2.918181655155912\n",
      "R2 score of Means after Epoch 1 : 0.9\n",
      "Total variance loss is=>  7.268667138969127\n",
      "R2 score of Variance after Epoch 1 : -2.66\n",
      "Accuracy of allocation after Epoch 1 : 82.0%\n",
      "\n",
      "\n",
      "Epoch 2 : prior probability =[0.31282479 0.21029489 0.47688032]\n",
      "Epoch 2 : mean =[0.15939084 4.19373488 8.79382317]\n",
      "Epoch 2 : variance =[1.69607986 1.98089977 5.93925414]\n",
      "Total mean loss is=>  2.1718327879971953\n",
      "R2 score of Means after Epoch 2 : 0.94\n",
      "Total variance loss is=>  6.616233771749796\n",
      "R2 score of Variance after Epoch 2 : -1.3\n",
      "Accuracy of allocation after Epoch 2 : 87.33%\n",
      "\n",
      "\n",
      "Epoch 3 : prior probability =[0.31543254 0.23150576 0.4530617 ]\n",
      "Epoch 3 : mean =[0.07379184 4.49184198 8.9661518 ]\n",
      "Epoch 3 : variance =[1.2799619  1.30510406 5.65629681]\n",
      "Total mean loss is=>  1.6157980579320954\n",
      "R2 score of Means after Epoch 3 : 0.97\n",
      "Total variance loss is=>  5.241362763757981\n",
      "R2 score of Variance after Epoch 3 : -0.72\n",
      "Accuracy of allocation after Epoch 3 : 92.67%\n",
      "\n",
      "\n",
      "Epoch 4 : prior probability =[0.32254878 0.25271038 0.42474084]\n",
      "Epoch 4 : mean =[0.0846146  4.66175612 9.22919766]\n",
      "Epoch 4 : variance =[1.21360815 0.87620097 4.89251452]\n",
      "Total mean loss is=>  1.1936608229713035\n",
      "R2 score of Means after Epoch 4 : 0.98\n",
      "Total variance loss is=>  4.229921693000679\n",
      "R2 score of Variance after Epoch 4 : -0.53\n",
      "Accuracy of allocation after Epoch 4 : 95.33%\n",
      "\n",
      "\n",
      "Epoch 5 : prior probability =[0.33002264 0.2723985  0.39757887]\n",
      "Epoch 5 : mean =[0.12457083 4.76652028 9.52233578]\n",
      "Epoch 5 : variance =[1.25901622 0.66766343 3.82820308]\n",
      "Total mean loss is=>  0.8357147605487053\n",
      "R2 score of Means after Epoch 5 : 0.99\n",
      "Total variance loss is=>  3.4195558750990482\n",
      "R2 score of Variance after Epoch 5 : -0.45\n",
      "Accuracy of allocation after Epoch 5 : 96.67%\n",
      "\n",
      "\n",
      "Epoch 6 : prior probability =[0.33373587 0.29177399 0.37449014]\n",
      "Epoch 6 : mean =[0.14857553 4.82771498 9.79250616]\n",
      "Epoch 6 : variance =[1.30021596 0.61002313 2.75862856]\n",
      "Total mean loss is=>  0.5283543947175855\n",
      "R2 score of Means after Epoch 6 : 1.0\n",
      "Total variance loss is=>  2.44882138777416\n",
      "R2 score of Variance after Epoch 6 : -0.39\n",
      "Accuracy of allocation after Epoch 6 : 98.0%\n",
      "\n",
      "\n",
      "Epoch 7 : prior probability =[0.33485303 0.30901146 0.35613552]\n",
      "Epoch 7 : mean =[ 0.15744286  4.88187359 10.00773124]\n",
      "Epoch 7 : variance =[1.32100075 0.63054517 1.92353322]\n",
      "Total mean loss is=>  0.2833005057127198\n",
      "R2 score of Means after Epoch 7 : 1.0\n",
      "Total variance loss is=>  1.613988800268352\n",
      "R2 score of Variance after Epoch 7 : -0.3\n",
      "Accuracy of allocation after Epoch 7 : 98.67%\n",
      "\n",
      "\n",
      "Epoch 8 : prior probability =[0.33509291 0.32172571 0.34318137]\n",
      "Epoch 8 : mean =[ 0.15975845  4.93500496 10.15244994]\n",
      "Epoch 8 : variance =[1.32768251 0.68378333 1.40576147]\n",
      "Total mean loss is=>  0.37720342473003865\n",
      "R2 score of Means after Epoch 8 : 1.0\n",
      "Total variance loss is=>  1.0496606420209882\n",
      "R2 score of Variance after Epoch 8 : -0.18\n",
      "Accuracy of allocation after Epoch 8 : 99.33%\n"
     ]
    }
   ],
   "source": [
    "X,varT=createdata(1)\n",
    "means,var,wprior,posterior=initialize(X,3)\n",
    "wprior,means,var,totalaccuracy,li=GMM(X,3,varT,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Accuracy after last epoch :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy of allocation : 99.33%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  50 49 51\n"
     ]
    }
   ],
   "source": [
    "finalaccuracy = round((accuracy_score(li, Y)*100),2)\n",
    "print(\"Final Accuracy of allocation : {}%\".format(finalaccuracy))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li.count(0),li.count(1),li.count(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initially each of the Gaussians consisted of 50 data points. Here we can see that it successfully predicted and classified the total 150 data points in 3 respective Gaussians(0,1,2). \n",
    "* Here I ran my algorithm for 8 epochs since I noticed that after 8th epoch my algorithm started to show no change in parameters which are mean, variance and prior probabilities. \n",
    "* I had initilized the parameters such as mean, variance, and prior probability with some random values for computation. Then at the Expectation Step, at each iteration I calculated posterior probabilities(expectations) for each of the inputs to compute probability(expectation) of inputs belonging to each of the clusters where posterior probabilites were computed using likelihood and prior probabilities. \n",
    "* Then after getting 3 posterior probablilities for each of the 150 data inputs, by taking argmax of the porbabilities, I calculated the corresponding indexes which are in turn the corresponding Gaussians 0,1,2. Accuracy computation was performed at each step to cacluate the accuracy of classfied samples of data. Here I received an accuracy of 99.33% after 8th epoch. \n",
    "* In the Maximization step, parameters (mean, variance and prior probability) were updated at each iteration by using the dedicated formulas given above. In this way we are trying to maximize our likelihood in the next iteration which in turn maximizes our Posterior probability thus attempting to classify more accurately giving better results. \n",
    "* Mean and Variance Losses are computed at each step and corresponding R2 score was computed to measure the perfomance of our algorithm by evaluating scores for Mean and Variances for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different variances :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For variance 4, that is S.D = 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 : prior probability =[0.30648201 0.19277336 0.50074463]\n",
      "Epoch 1 : mean =[0.11791059 3.38532905 9.1104891 ]\n",
      "Epoch 1 : variance =[4.84651547 1.70415693 7.15811978]\n",
      "Total mean loss is=>  2.6220924308400537\n",
      "R2 score of Means after Epoch 1 : 0.92\n",
      "Total variance loss is=>  8.300478314774864\n",
      "R2 score of Variance after Epoch 1 : -1.32\n",
      "Accuracy of allocation after Epoch 1 : 74.0%\n",
      "\n",
      "\n",
      "Epoch 2 : prior probability =[0.30015127 0.20105465 0.49879409]\n",
      "Epoch 2 : mean =[0.07929244 3.60182426 9.0273805 ]\n",
      "Epoch 2 : variance =[5.18696086 1.76368951 8.02721052]\n",
      "Total mean loss is=>  2.4500876773122244\n",
      "R2 score of Means after Epoch 2 : 0.93\n",
      "Total variance loss is=>  9.450481866591133\n",
      "R2 score of Variance after Epoch 2 : -1.37\n",
      "Accuracy of allocation after Epoch 2 : 74.67%\n",
      "\n",
      "\n",
      "Epoch 3 : prior probability =[0.30016688 0.20298775 0.49684537]\n",
      "Epoch 3 : mean =[0.06473037 3.72630442 9.006712  ]\n",
      "Epoch 3 : variance =[5.11564422 1.81407791 8.34464504]\n",
      "Total mean loss is=>  2.3317139492065877\n",
      "R2 score of Means after Epoch 3 : 0.94\n",
      "Total variance loss is=>  9.646211342121846\n",
      "R2 score of Variance after Epoch 3 : -1.34\n",
      "Accuracy of allocation after Epoch 3 : 75.33%\n",
      "\n",
      "\n",
      "Epoch 4 : prior probability =[0.30100906 0.20426852 0.49472242]\n",
      "Epoch 4 : mean =[0.05461028 3.81320253 9.00588215]\n",
      "Epoch 4 : variance =[5.0144618  1.84069466 8.47494015]\n",
      "Total mean loss is=>  2.235525591037579\n",
      "R2 score of Means after Epoch 4 : 0.94\n",
      "Total variance loss is=>  9.648707281329527\n",
      "R2 score of Variance after Epoch 4 : -1.32\n",
      "Accuracy of allocation after Epoch 4 : 76.67%\n",
      "\n",
      "\n",
      "Epoch 5 : prior probability =[0.30192674 0.20567176 0.49240151]\n",
      "Epoch 5 : mean =[0.04827041 3.87654257 9.01479328]\n",
      "Epoch 5 : variance =[4.93533785 1.85144867 8.52153779]\n",
      "Total mean loss is=>  2.1569345640755033\n",
      "R2 score of Means after Epoch 5 : 0.94\n",
      "Total variance loss is=>  9.605426956988971\n",
      "R2 score of Variance after Epoch 5 : -1.3\n",
      "Accuracy of allocation after Epoch 5 : 77.33%\n",
      "\n",
      "\n",
      "Epoch 6 : prior probability =[0.30282891 0.20718617 0.48998492]\n",
      "Epoch 6 : mean =[0.04531315 3.92364607 9.0290941 ]\n",
      "Epoch 6 : variance =[4.88049136 1.85412678 8.52264415]\n",
      "Total mean loss is=>  2.0925729879485715\n",
      "R2 score of Means after Epoch 6 : 0.95\n",
      "Total variance loss is=>  9.549008733885625\n",
      "R2 score of Variance after Epoch 6 : -1.28\n",
      "Accuracy of allocation after Epoch 6 : 78.0%\n",
      "\n",
      "\n",
      "Epoch 7 : prior probability =[0.30371544 0.20872756 0.48755699]\n",
      "Epoch 7 : mean =[0.04509231 3.95938812 9.04640615]\n",
      "Epoch 7 : variance =[4.84479554 1.85356932 8.4965892 ]\n",
      "Total mean loss is=>  2.039298043952013\n",
      "R2 score of Means after Epoch 7 : 0.95\n",
      "Total variance loss is=>  9.48781542262882\n",
      "R2 score of Variance after Epoch 7 : -1.27\n",
      "Accuracy of allocation after Epoch 7 : 78.0%\n",
      "\n",
      "\n",
      "Epoch 8 : prior probability =[0.30458895 0.21024359 0.48516747]\n",
      "Epoch 8 : mean =[0.04693948 3.98718875 9.06530104]\n",
      "Epoch 8 : variance =[4.82317923 1.85236342 8.45402865]\n",
      "Total mean loss is=>  1.9944496791568558\n",
      "R2 score of Means after Epoch 8 : 0.95\n",
      "Total variance loss is=>  9.424844462997914\n",
      "R2 score of Variance after Epoch 8 : -1.27\n",
      "Accuracy of allocation after Epoch 8 : 78.0%\n",
      "\n",
      "\n",
      "Epoch 9 : prior probability =[0.30544692 0.21171138 0.48284171]\n",
      "Epoch 9 : mean =[0.05027552 4.00945093 9.08489116]\n",
      "Epoch 9 : variance =[4.8116903  1.85176295 8.40176575]\n",
      "Total mean loss is=>  1.955933431938094\n",
      "R2 score of Means after Epoch 9 : 0.96\n",
      "Total variance loss is=>  9.361693093573415\n",
      "R2 score of Variance after Epoch 9 : -1.27\n",
      "Accuracy of allocation after Epoch 9 : 78.0%\n",
      "\n",
      "\n",
      "Epoch 10 : prior probability =[0.30628457 0.21312434 0.48059109]\n",
      "Epoch 10 : mean =[0.05463864 4.02784822 9.10462113]\n",
      "Epoch 10 : variance =[4.80741115 1.8523032  8.34427187]\n",
      "Total mean loss is=>  1.9221692833522397\n",
      "R2 score of Means after Epoch 10 : 0.96\n",
      "Total variance loss is=>  9.299379816430967\n",
      "R2 score of Variance after Epoch 10 : -1.28\n",
      "Accuracy of allocation after Epoch 10 : 78.67%\n",
      "Final Accuracy of allocation : 78.67%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  43 38 69\n"
     ]
    }
   ],
   "source": [
    "X2,varT2=createdata(2)\n",
    "means,var,wprior,posterior=initialize(X2,3)\n",
    "wprior,means,var,totalaccuracy2,li2=GMM(X2,3,varT2,10)\n",
    "finalaccuracy2 = round((accuracy_score(li2, Y)*100),2)\n",
    "print(\"Final Accuracy of allocation : {}%\".format(finalaccuracy2))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li2.count(0),li2.count(1),li2.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can see that as we increased the variance, algorithm performed comparitively poorly than the case where we used a smaller variance = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For variance 6.25, that is S.D = 2.5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 : prior probability =[0.30394701 0.17849251 0.51756048]\n",
      "Epoch 1 : mean =[-0.13059147  3.3530637   9.18666587]\n",
      "Epoch 1 : variance =[6.03227425 1.76626288 8.75574925]\n",
      "Total mean loss is=>  2.5908618955947826\n",
      "R2 score of Means after Epoch 1 : 0.92\n",
      "Total variance loss is=>  10.521760626607335\n",
      "R2 score of Variance after Epoch 1 : -1.1\n",
      "Accuracy of allocation after Epoch 1 : 70.0%\n",
      "\n",
      "\n",
      "Epoch 2 : prior probability =[0.29608897 0.18216481 0.52174622]\n",
      "Epoch 2 : mean =[-0.1214891   3.53579902  9.01843135]\n",
      "Epoch 2 : variance =[ 6.80670271  2.01526533 10.10607441]\n",
      "Total mean loss is=>  2.5672587306452894\n",
      "R2 score of Means after Epoch 2 : 0.93\n",
      "Total variance loss is=>  12.397511783434702\n",
      "R2 score of Variance after Epoch 2 : -1.32\n",
      "Accuracy of allocation after Epoch 2 : 69.33%\n",
      "\n",
      "\n",
      "Epoch 3 : prior probability =[0.29498638 0.18150557 0.52350805]\n",
      "Epoch 3 : mean =[-0.09244643  3.65119205  8.93590429]\n",
      "Epoch 3 : variance =[ 7.0269649   2.27399627 10.8148594 ]\n",
      "Total mean loss is=>  2.5053500839525595\n",
      "R2 score of Means after Epoch 3 : 0.93\n",
      "Total variance loss is=>  13.067828033275175\n",
      "R2 score of Variance after Epoch 3 : -1.45\n",
      "Accuracy of allocation after Epoch 3 : 68.67%\n",
      "\n",
      "\n",
      "Epoch 4 : prior probability =[0.29504538 0.18095934 0.52399528]\n",
      "Epoch 4 : mean =[-0.06793322  3.74215913  8.8861942 ]\n",
      "Epoch 4 : variance =[ 7.13206632  2.50897904 11.26159038]\n",
      "Total mean loss is=>  2.439579887724582\n",
      "R2 score of Means after Epoch 4 : 0.93\n",
      "Total variance loss is=>  13.402635743751938\n",
      "R2 score of Variance after Epoch 4 : -1.56\n",
      "Accuracy of allocation after Epoch 4 : 70.0%\n",
      "\n",
      "\n",
      "Epoch 5 : prior probability =[0.29541726 0.1809881  0.52359464]\n",
      "Epoch 5 : mean =[-0.04703511  3.81765     8.85495082]\n",
      "Epoch 5 : variance =[ 7.20304719  2.71594326 11.57144288]\n",
      "Total mean loss is=>  2.374434281768581\n",
      "R2 score of Means after Epoch 5 : 0.93\n",
      "Total variance loss is=>  13.99043332225966\n",
      "R2 score of Variance after Epoch 5 : -1.66\n",
      "Accuracy of allocation after Epoch 5 : 70.0%\n",
      "\n",
      "\n",
      "Epoch 6 : prior probability =[0.29592293 0.18148173 0.52259534]\n",
      "Epoch 6 : mean =[-0.02846536  3.88153085  8.83562356]\n",
      "Epoch 6 : variance =[ 7.26074415  2.89678563 11.79663554]\n",
      "Total mean loss is=>  2.3113109495799686\n",
      "R2 score of Means after Epoch 6 : 0.93\n",
      "Total variance loss is=>  14.454165322840526\n",
      "R2 score of Variance after Epoch 6 : -1.76\n",
      "Accuracy of allocation after Epoch 6 : 69.33%\n",
      "\n",
      "\n",
      "Epoch 7 : prior probability =[0.29651458 0.182291   0.52119441]\n",
      "Epoch 7 : mean =[-0.01148011  3.93618575  8.82459926]\n",
      "Epoch 7 : variance =[ 7.31160442  3.05459154 11.96355573]\n",
      "Total mean loss is=>  2.2506951023314326\n",
      "R2 score of Means after Epoch 7 : 0.94\n",
      "Total variance loss is=>  14.829751698621987\n",
      "R2 score of Variance after Epoch 7 : -1.85\n",
      "Accuracy of allocation after Epoch 7 : 70.0%\n",
      "\n",
      "\n",
      "Epoch 8 : prior probability =[0.29717383 0.18330383 0.51952234]\n",
      "Epoch 8 : mean =[4.33529915e-03 3.98333367e+00 8.81965995e+00]\n",
      "Epoch 8 : variance =[ 7.35796232  3.19239453 12.08758878]\n",
      "Total mean loss is=>  2.2013416729610755\n",
      "R2 score of Means after Epoch 8 : 0.94\n",
      "Total variance loss is=>  15.137945639318815\n",
      "R2 score of Variance after Epoch 8 : -1.93\n",
      "Accuracy of allocation after Epoch 8 : 70.0%\n",
      "\n",
      "\n",
      "Epoch 9 : prior probability =[0.2978895  0.18444489 0.51766561]\n",
      "Epoch 9 : mean =[0.01922593 4.02430258 8.81934154]\n",
      "Epoch 9 : variance =[ 7.40083046  3.3128921  12.17868309]\n",
      "Total mean loss is=>  2.1755818146909283\n",
      "R2 score of Means after Epoch 9 : 0.94\n",
      "Total variance loss is=>  15.392405658340321\n",
      "R2 score of Variance after Epoch 9 : -2.01\n",
      "Accuracy of allocation after Epoch 9 : 70.67%\n",
      "\n",
      "\n",
      "Epoch 10 : prior probability =[0.29865262 0.18566531 0.51568207]\n",
      "Epoch 10 : mean =[0.03334899 4.06015373 8.82262512]\n",
      "Epoch 10 : variance =[ 7.4407531   3.41841679 12.24376132]\n",
      "Total mean loss is=>  2.1505701331919416\n",
      "R2 score of Means after Epoch 10 : 0.94\n",
      "Total variance loss is=>  15.602931213711624\n",
      "R2 score of Variance after Epoch 10 : -2.08\n",
      "Accuracy of allocation after Epoch 10 : 70.67%\n",
      "Final Accuracy of allocation : 70.67%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  46 26 78\n"
     ]
    }
   ],
   "source": [
    "X3,varT3=createdata(2.5)\n",
    "means,var,wprior,posterior=initialize(X3,3)\n",
    "wprior,means,var,totalaccuracy3,li3=GMM(X3,3,varT3,10)\n",
    "finalaccuracy3 = round((accuracy_score(li3, Y)*100),2)\n",
    "print(\"Final Accuracy of allocation : {}%\".format(finalaccuracy3))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li3.count(0),li3.count(1),li3.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 4 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For variance 9, that is S.D = 3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 : prior probability =[0.30379591 0.16129193 0.53491216]\n",
      "Epoch 1 : mean =[-0.41752964  3.32022977  9.28651244]\n",
      "Epoch 1 : variance =[ 7.26556547  1.90157733 10.5107976 ]\n",
      "Total mean loss is=>  2.8107874259869305\n",
      "R2 score of Means after Epoch 1 : 0.93\n",
      "Total variance loss is=>  12.874785747807543\n",
      "R2 score of Variance after Epoch 1 : -1.01\n",
      "Accuracy of allocation after Epoch 1 : 66.0%\n",
      "\n",
      "\n",
      "Epoch 2 : prior probability =[0.29441945 0.16233055 0.54325   ]\n",
      "Epoch 2 : mean =[-0.36020809  3.49769224  9.0463338 ]\n",
      "Epoch 2 : variance =[ 8.55281883  2.37076113 12.35750922]\n",
      "Total mean loss is=>  2.8161820496327623\n",
      "R2 score of Means after Epoch 2 : 0.93\n",
      "Total variance loss is=>  15.539566912639172\n",
      "R2 score of Variance after Epoch 2 : -1.34\n",
      "Accuracy of allocation after Epoch 2 : 66.0%\n",
      "\n",
      "\n",
      "Epoch 3 : prior probability =[0.29188295 0.16174235 0.54637471]\n",
      "Epoch 3 : mean =[-0.28670838  3.63323711  8.91730128]\n",
      "Epoch 3 : variance =[ 9.14176108  2.86790262 13.47022188]\n",
      "Total mean loss is=>  2.736169992611626\n",
      "R2 score of Means after Epoch 3 : 0.93\n",
      "Total variance loss is=>  16.74408034069482\n",
      "R2 score of Variance after Epoch 3 : -1.59\n",
      "Accuracy of allocation after Epoch 3 : 66.0%\n",
      "\n",
      "\n",
      "Epoch 4 : prior probability =[0.29075054 0.16222643 0.54702303]\n",
      "Epoch 4 : mean =[-0.22635962  3.75512384  8.83470065]\n",
      "Epoch 4 : variance =[ 9.52003048  3.3276667  14.24320056]\n",
      "Total mean loss is=>  2.636535134905623\n",
      "R2 score of Means after Epoch 4 : 0.93\n",
      "Total variance loss is=>  18.09089774728308\n",
      "R2 score of Variance after Epoch 4 : -1.82\n",
      "Accuracy of allocation after Epoch 4 : 66.0%\n",
      "\n",
      "\n",
      "Epoch 5 : prior probability =[0.29019628 0.16353786 0.54626586]\n",
      "Epoch 5 : mean =[-0.17720568  3.86468027  8.77879096]\n",
      "Epoch 5 : variance =[ 9.79337741  3.72686473 14.81966464]\n",
      "Total mean loss is=>  2.5337344531692225\n",
      "R2 score of Means after Epoch 5 : 0.93\n",
      "Total variance loss is=>  19.33990678048925\n",
      "R2 score of Variance after Epoch 5 : -2.02\n",
      "Accuracy of allocation after Epoch 5 : 66.0%\n",
      "\n",
      "\n",
      "Epoch 6 : prior probability =[0.29000191 0.1652666  0.54473148]\n",
      "Epoch 6 : mean =[-0.13666907  3.96190573  8.74011254]\n",
      "Epoch 6 : variance =[10.00064568  4.06134011 15.26445817]\n",
      "Total mean loss is=>  2.4346507950557035\n",
      "R2 score of Means after Epoch 6 : 0.93\n",
      "Total variance loss is=>  20.32644395424027\n",
      "R2 score of Variance after Epoch 6 : -2.19\n",
      "Accuracy of allocation after Epoch 6 : 65.33%\n",
      "\n",
      "\n",
      "Epoch 7 : prior probability =[0.29007416 0.16714419 0.54278164]\n",
      "Epoch 7 : mean =[-0.10282559  4.04747031  8.71338745]\n",
      "Epoch 7 : variance =[10.16130075  4.33593492 15.6133516 ]\n",
      "Total mean loss is=>  2.341967829881911\n",
      "R2 score of Means after Epoch 7 : 0.93\n",
      "Total variance loss is=>  21.110587274471147\n",
      "R2 score of Variance after Epoch 7 : -2.34\n",
      "Accuracy of allocation after Epoch 7 : 65.33%\n",
      "\n",
      "\n",
      "Epoch 8 : prior probability =[0.29035607 0.16902279 0.54062114]\n",
      "Epoch 8 : mean =[-0.07424928  4.12254496  8.69537874]\n",
      "Epoch 8 : variance =[10.28721086  4.55885326 15.88900999]\n",
      "Total mean loss is=>  2.256325571534987\n",
      "R2 score of Means after Epoch 8 : 0.94\n",
      "Total variance loss is=>  21.735074116522465\n",
      "R2 score of Variance after Epoch 8 : -2.45\n",
      "Accuracy of allocation after Epoch 8 : 65.33%\n",
      "\n",
      "\n",
      "Epoch 9 : prior probability =[0.29080646 0.17082925 0.53836429]\n",
      "Epoch 9 : mean =[-0.0498505   4.18847478  8.68395957]\n",
      "Epoch 9 : variance =[10.38649518  4.73869858 16.10716876]\n",
      "Total mean loss is=>  2.1774161446507625\n",
      "R2 score of Means after Epoch 9 : 0.94\n",
      "Total variance loss is=>  22.232362520269618\n",
      "R2 score of Variance after Epoch 9 : -2.55\n",
      "Accuracy of allocation after Epoch 9 : 65.33%\n",
      "\n",
      "\n",
      "Epoch 10 : prior probability =[0.29139376 0.17253244 0.5360738 ]\n",
      "Epoch 10 : mean =[-0.02877784  4.24657061  8.67765854]\n",
      "Epoch 10 : variance =[10.46507399  4.88319575 16.27938632]\n",
      "Total mean loss is=>  2.1045486918090752\n",
      "R2 score of Means after Epoch 10 : 0.94\n",
      "Total variance loss is=>  22.627656061022144\n",
      "R2 score of Variance after Epoch 10 : -2.63\n",
      "Accuracy of allocation after Epoch 10 : 65.33%\n",
      "Final Accuracy of allocation : 65.33%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  48 18 84\n"
     ]
    }
   ],
   "source": [
    "X4,varT4=createdata(3)\n",
    "means,var,wprior,posterior=initialize(X4,3)\n",
    "wprior,means,var,totalaccuracy4,li4 = GMM(X4,3,varT4,10)\n",
    "finalaccuracy4 = round((accuracy_score(li4, Y)*100),2)\n",
    "print(\"Final Accuracy of allocation : {}%\".format(finalaccuracy4))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li4.count(0),li4.count(1),li4.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on  tweaking variance :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see a decrease in accuracies when we ran our algorithm for less number of epochs. In our algorithm, we stopped when our posterior probabilities showed almost no change. After trying different values for variance, we noticed that as variance increases, our GMM algorithm takes many iterations to converge. Our algorithm converges slowly because it has a greater variance in data hence it becomes compartively difficult for the algorithm to correctly classify the data into respected Gaussians having means (0,5,10)\n",
    "* Advantage of using GMM is that it is much more flexible allowing us to generate much better fitting distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
