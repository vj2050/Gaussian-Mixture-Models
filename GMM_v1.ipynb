{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#from random import normal\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating gaussian distributions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdata(v):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    dis1=np.random.normal(0,v,50)\n",
    "    dis2=np.random.normal(5,v,50)\n",
    "    dis3=np.random.normal(10,v,50)\n",
    "    A=np.concatenate([dis1,dis2])\n",
    "    X=np.concatenate([A,dis3])\n",
    "    X\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=createdata(1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef plot_hist(data):\\n    plt.figure(figsize=(15,7))\\n    for x in data:\\n        plt.hist(x, bins = 80, normed = True, alpha = 0.7)\\n    #plt.xlim(-10, 20)\\n    plt.figure(figsize=(15,7))\\n    \\nplot_hist(X)\\n'"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def plot_hist(data):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    for x in data:\n",
    "        plt.hist(x, bins = 80, normed = True, alpha = 0.7)\n",
    "    #plt.xlim(-10, 20)\n",
    "    plt.figure(figsize=(15,7))\n",
    "    \n",
    "plot_hist(X)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to initialize random means, variances and prior probabilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X,K):\n",
    "    \n",
    "    m=np.array([2.,3.,7.])             #random means\n",
    "    #m=np.random.choice(X,size=3)      \n",
    "    v=np.ones((3))                     #random variances set to 1 initially\n",
    "    w=np.ones((K))/K                   #random prior weights set to equal\n",
    "    #posterior probability that each input belongs to each of the 3 gaussians\n",
    "    posterior=np.zeros((len(X),K),dtype=float)\n",
    "    return m,v,w,posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 3., 7.]),\n",
       " array([1., 1., 1.]),\n",
       " array([0.33333333, 0.33333333, 0.33333333]),\n",
       " (150, 3))"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means,var,wprior,posterior=initialize(X,K)\n",
    "means,var,wprior,posterior.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True assignment of data samples to respective Gaussians and means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[0]*50 + [1]*50 + [2]*50\n",
    "Y\n",
    "M=[0,5,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to allocate the probability of each of the data points belonging to each one of the clusters(Soft Allocation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 10]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM(X,K,epochs):\n",
    "    N=len(X)\n",
    "    #K=3\n",
    "    pi=math.pi\n",
    "    iterations=epochs\n",
    "    M=np.array([0,5,10])\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        \n",
    "        #Expection Step : Calculating Posterior probabilities for \n",
    "        for i in range(len(X)):\n",
    "            for k in range(K):\n",
    "\n",
    "                part1=1/math.sqrt(2*pi*(var[k]))\n",
    "                part2=(-1/2)*((X[i]-means[k])**2)\n",
    "                part3= part2/(var[k])\n",
    "                part4=np.exp(part3)\n",
    "                posterior[i][k]=part1 * part4\n",
    "\n",
    "                #posterior[i][k]=wprior[k]*(1/math.sqrt(2*var[k]*pi))*(math.exp(-(1/2)*((X[i]-means[k])**2)))\n",
    "\n",
    "            posterior[i]=posterior[i]/np.sum(posterior[i])\n",
    "        #print(posterior[0:2])\n",
    "        \n",
    "        #Maximization Step : Updating parameters : mean, variance, and prior probabilities :\n",
    "        for i in range(K):\n",
    "            wprior[i]=np.sum(posterior[:,i])/N\n",
    "            means[i]=np.dot(posterior[:,i],X[:].T)/(np.sum(posterior[:,i]))\n",
    "            var[i]=np.dot(posterior[:,i],((X-means[i])**2).T)/(np.sum(posterior[:,i]))\n",
    "        \n",
    "        \n",
    "        li=[]\n",
    "        for i in range(len(X)):\n",
    "            li.append(np.argmax(posterior[i]))\n",
    "        print(\"Epoch \" + str(iter) + \" : \" + \"priorprob=\"+ str(wprior))\n",
    "        print(\"Epoch \" + str(iter) + \" : \" + \"mean=\"+ str(means))\n",
    "        print(\"Epoch \" + str(iter) + \" : \"+ \"var=\" + str(var))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #accumean=accuracy_score(means,M)\n",
    "        #print(\"Accuracy of means after Epoch {} : {}%\".format(iter+1, accumean))\n",
    "        accuracy = round((accuracy_score(li, li1)*100),2)\n",
    "        print(\"Accuracy of allocation after Epoch {} : {}%\".format(iter+1, accuracy))\n",
    "        accuracy\n",
    "        '''\n",
    "        diff=[]\n",
    "        for m in range(len(means)):\n",
    "            diff[m]=(means[m].T-M[m].T)\n",
    "        print(diff)    \n",
    "        '''\n",
    "        print(means.shape, M.shape)    \n",
    "            \n",
    "\n",
    "    return (wprior, means,var,accuracy,li)\n",
    "    #means.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : priorprob=[0.34189327 0.34202421 0.31608253]\n",
      "Epoch 0 : mean=[ 2.28926901  3.17016952 10.30877374]\n",
      "Epoch 0 : var=[7.40977473 7.84458953 1.03734907]\n",
      "Accuracy of allocation after Epoch 1 : 70.67%\n",
      "(3,) (3,)\n",
      "Epoch 1 : priorprob=[0.34189534 0.34203174 0.31607292]\n",
      "Epoch 1 : mean=[ 2.28310396  3.17652176 10.30879134]\n",
      "Epoch 1 : var=[7.4023968  7.84204846 1.03733464]\n",
      "Accuracy of allocation after Epoch 2 : 70.67%\n",
      "(3,) (3,)\n",
      "Epoch 2 : priorprob=[0.34189743 0.34203956 0.31606301]\n",
      "Epoch 2 : mean=[ 2.27678737  3.18303087 10.30880955]\n",
      "Epoch 2 : var=[7.3947869  7.83933851 1.03731973]\n",
      "Accuracy of allocation after Epoch 3 : 70.67%\n",
      "(3,) (3,)\n",
      "Epoch 3 : priorprob=[0.34189952 0.34204768 0.3160528 ]\n",
      "Epoch 3 : mean=[ 2.27031251  3.18970378 10.3088284 ]\n",
      "Epoch 3 : var=[7.38693325 7.83644832 1.03730434]\n",
      "Accuracy of allocation after Epoch 4 : 70.67%\n",
      "(3,) (3,)\n"
     ]
    }
   ],
   "source": [
    "wprior,means,var,accuracy,li=GMM(X,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.30488337e-01, 4.69511663e-01, 9.38748785e-14],\n",
       "       [5.48374064e-01, 4.51625936e-01, 3.38982576e-18],\n",
       "       [5.41607348e-01, 4.58392652e-01, 3.05413108e-16],\n",
       "       [5.22646020e-01, 4.77353980e-01, 2.45478529e-12],\n",
       "       [5.28856150e-01, 4.71143850e-01, 1.93288952e-13],\n",
       "       [5.59657211e-01, 4.40342789e-01, 2.88766621e-23],\n",
       "       [5.41970811e-01, 4.58029189e-01, 2.45770702e-16],\n",
       "       [5.53704763e-01, 4.46295237e-01, 3.72096971e-20],\n",
       "       [5.53282849e-01, 4.46717151e-01, 5.56457836e-20],\n",
       "       [5.48262623e-01, 4.51737377e-01, 3.68441950e-18],\n",
       "       [5.50985071e-01, 4.49014929e-01, 4.28441857e-19],\n",
       "       [5.35141116e-01, 4.64858884e-01, 1.03281226e-14],\n",
       "       [5.44294949e-01, 4.55705051e-01, 5.77473590e-17],\n",
       "       [5.51201943e-01, 4.48798057e-01, 3.56841360e-19],\n",
       "       [5.47904974e-01, 4.52095026e-01, 4.80227496e-18],\n",
       "       [5.49074458e-01, 4.50925542e-01, 1.99041710e-18],\n",
       "       [5.34562725e-01, 4.65437275e-01, 1.37673453e-14],\n",
       "       [5.54166510e-01, 4.45833490e-01, 2.36850180e-20],\n",
       "       [5.49288326e-01, 4.50711674e-01, 1.68653480e-18],\n",
       "       [5.58923336e-01, 4.41076664e-01, 8.66494770e-23],\n",
       "       [5.64312476e-01, 4.35687524e-01, 8.77036220e-30],\n",
       "       [5.45558127e-01, 4.54441873e-01, 2.50734594e-17],\n",
       "       [5.43039769e-01, 4.56960231e-01, 1.27914975e-16],\n",
       "       [5.58209785e-01, 4.41790215e-01, 2.32988430e-22],\n",
       "       [5.22144873e-01, 4.77855127e-01, 2.97543344e-12],\n",
       "       [5.61992189e-01, 4.38007811e-01, 3.69887939e-25],\n",
       "       [5.51924624e-01, 4.48075376e-01, 1.91333595e-19],\n",
       "       [5.54013395e-01, 4.45986605e-01, 2.75496311e-20],\n",
       "       [5.33994893e-01, 4.66005107e-01, 1.81862376e-14],\n",
       "       [5.34922597e-01, 4.65077403e-01, 1.15182331e-14],\n",
       "       [5.50878703e-01, 4.49121297e-01, 4.68326380e-19],\n",
       "       [5.48607534e-01, 4.51392466e-01, 2.84334156e-18],\n",
       "       [5.59129398e-01, 4.40870602e-01, 6.42263229e-23],\n",
       "       [5.63635062e-01, 4.36364938e-01, 2.50233944e-27],\n",
       "       [5.55341613e-01, 4.44658387e-01, 7.07291357e-21],\n",
       "       [5.50865000e-01, 4.49135000e-01, 4.73712407e-19],\n",
       "       [5.38288694e-01, 4.61711306e-01, 2.00672549e-15],\n",
       "       [5.38668196e-01, 4.61331804e-01, 1.63206052e-15],\n",
       "       [5.55653255e-01, 4.44346745e-01, 5.05324889e-21],\n",
       "       [5.54974076e-01, 4.45025924e-01, 1.04226533e-20],\n",
       "       [5.60057265e-01, 4.39942735e-01, 1.52153437e-23],\n",
       "       [5.61850987e-01, 4.38149013e-01, 5.08938025e-25],\n",
       "       [5.62900373e-01, 4.37099627e-01, 3.47133994e-26],\n",
       "       [5.27515702e-01, 4.72484298e-01, 3.43543585e-13],\n",
       "       [5.56585200e-01, 4.43414800e-01, 1.76730944e-21],\n",
       "       [5.56046349e-01, 4.43953651e-01, 3.27227435e-21],\n",
       "       [5.61103948e-01, 4.38896052e-01, 2.37801474e-24],\n",
       "       [5.44097805e-01, 4.55902195e-01, 6.55708771e-17],\n",
       "       [5.62593368e-01, 4.37406632e-01, 8.30955462e-26],\n",
       "       [5.54230752e-01, 4.45769248e-01, 2.22205339e-20],\n",
       "       [4.84118786e-01, 5.15881034e-01, 1.79835956e-07],\n",
       "       [4.50484343e-01, 5.49420809e-01, 9.48482243e-05],\n",
       "       [4.74629669e-01, 5.25368997e-01, 1.33355012e-06],\n",
       "       [4.90818990e-01, 5.09180972e-01, 3.80360660e-08],\n",
       "       [4.62000143e-01, 5.37985673e-01, 1.41840612e-05],\n",
       "       [4.49299290e-01, 5.50586829e-01, 1.13881359e-04],\n",
       "       [4.59427491e-01, 5.40550388e-01, 2.21201492e-05],\n",
       "       [4.52878300e-01, 5.47056610e-01, 6.50900112e-05],\n",
       "       [4.77732774e-01, 5.22266517e-01, 7.08936269e-07],\n",
       "       [4.70840379e-01, 5.29156817e-01, 2.80353824e-06],\n",
       "       [4.78680228e-01, 5.21319190e-01, 5.82001004e-07],\n",
       "       [4.70757960e-01, 5.29239192e-01, 2.84826076e-06],\n",
       "       [4.82131430e-01, 5.17868292e-01, 2.78580478e-07],\n",
       "       [5.02834819e-01, 4.97165179e-01, 1.65569650e-09],\n",
       "       [4.56374251e-01, 5.43588828e-01, 3.69203430e-05],\n",
       "       [4.71846833e-01, 5.28150858e-01, 2.30824478e-06],\n",
       "       [5.00796131e-01, 4.99203867e-01, 2.91991323e-09],\n",
       "       [4.48308465e-01, 5.51559073e-01, 1.32461448e-04],\n",
       "       [4.84402475e-01, 5.15597356e-01, 1.68804751e-07],\n",
       "       [4.59825401e-01, 5.40153932e-01, 2.06668166e-05],\n",
       "       [4.40462465e-01, 5.59123564e-01, 4.13971376e-04],\n",
       "       [4.57713253e-01, 5.42257197e-01, 2.95502610e-05],\n",
       "       [4.27388273e-01, 5.70445413e-01, 2.16631369e-03],\n",
       "       [4.92059829e-01, 5.07940143e-01, 2.81272332e-08],\n",
       "       [4.50043534e-01, 5.49854913e-01, 1.01552306e-04],\n",
       "       [4.78985941e-01, 5.21013513e-01, 5.45860579e-07],\n",
       "       [4.83525713e-01, 5.16474082e-01, 2.05143468e-07],\n",
       "       [4.76345684e-01, 5.23653373e-01, 9.42820088e-07],\n",
       "       [4.69512772e-01, 5.30483617e-01, 3.61149220e-06],\n",
       "       [4.59710245e-01, 5.40268677e-01, 2.10779151e-05],\n",
       "       [4.90462588e-01, 5.09537370e-01, 4.14450655e-08],\n",
       "       [4.35181110e-01, 5.63978832e-01, 8.40057120e-04],\n",
       "       [4.48225402e-01, 5.51640458e-01, 1.34140596e-04],\n",
       "       [4.98770629e-01, 5.01229366e-01, 5.05280888e-09],\n",
       "       [4.14097265e-01, 5.77891142e-01, 8.01159319e-03],\n",
       "       [3.91173819e-01, 5.76349529e-01, 3.24766525e-02],\n",
       "       [4.26028827e-01, 5.71448467e-01, 2.52270579e-03],\n",
       "       [4.66057525e-01, 5.33935608e-01, 6.86685438e-06],\n",
       "       [4.88271235e-01, 5.11728695e-01, 6.96863856e-08],\n",
       "       [4.30240972e-01, 5.68205494e-01, 1.55353361e-03],\n",
       "       [4.71882726e-01, 5.28114982e-01, 2.29220888e-06],\n",
       "       [4.24489468e-01, 5.72527762e-01, 2.98276999e-03],\n",
       "       [4.55517153e-01, 5.44440339e-01, 4.25076635e-05],\n",
       "       [4.32773745e-01, 5.66085962e-01, 1.14029290e-03],\n",
       "       [4.51353385e-01, 5.48563793e-01, 8.28222078e-05],\n",
       "       [4.41140288e-01, 5.58483021e-01, 3.76690956e-04],\n",
       "       [4.60953069e-01, 5.39029911e-01, 1.70203115e-05],\n",
       "       [3.98675765e-01, 5.78724779e-01, 2.25994567e-02],\n",
       "       [4.57770304e-01, 5.42200427e-01, 2.92692241e-05],\n",
       "       [4.50053603e-01, 5.49845002e-01, 1.01394371e-04],\n",
       "       [2.82187994e-03, 9.79373001e-03, 9.87384390e-01],\n",
       "       [6.90218428e-02, 1.32035358e-01, 7.98942800e-01],\n",
       "       [6.01657994e-02, 1.16542101e-01, 8.23292100e-01],\n",
       "       [3.50734349e-03, 1.01306065e-02, 9.86362050e-01],\n",
       "       [5.06004574e-02, 9.95821120e-02, 8.49817431e-01],\n",
       "       [2.83909229e-03, 9.97815446e-03, 9.87182753e-01],\n",
       "       [1.44391252e-02, 3.23124049e-02, 9.53248470e-01],\n",
       "       [2.43074445e-02, 5.13568567e-02, 9.24335699e-01],\n",
       "       [2.83238535e-03, 9.91180953e-03, 9.87255805e-01],\n",
       "       [2.89128808e-03, 9.24099519e-03, 9.87867717e-01],\n",
       "       [2.81861903e-03, 9.75082959e-03, 9.87430551e-01],\n",
       "       [3.63794852e-03, 1.03795107e-02, 9.85982541e-01],\n",
       "       [2.93925582e-02, 6.09179710e-02, 9.09689471e-01],\n",
       "       [2.82864105e-03, 9.87222998e-03, 9.87299129e-01],\n",
       "       [1.17323535e-02, 2.69353137e-02, 9.61332333e-01],\n",
       "       [3.88520604e-03, 1.08658035e-02, 9.85248990e-01],\n",
       "       [3.55133376e-03, 1.02136345e-02, 9.86235032e-01],\n",
       "       [1.00762336e-02, 2.36018410e-02, 9.66321925e-01],\n",
       "       [4.46244131e-03, 1.20398265e-02, 9.83497732e-01],\n",
       "       [3.60322387e-03, 1.03126601e-02, 9.86084116e-01],\n",
       "       [5.50189359e-03, 1.41969155e-02, 9.80301191e-01],\n",
       "       [4.44093723e-02, 8.84628096e-02, 8.67127818e-01],\n",
       "       [5.94386630e-03, 1.51166772e-02, 9.78939456e-01],\n",
       "       [3.00669038e-03, 9.31720396e-03, 9.87676106e-01],\n",
       "       [2.22969159e-02, 4.75349882e-02, 9.30168096e-01],\n",
       "       [1.00055421e-02, 2.34587390e-02, 9.66535719e-01],\n",
       "       [1.49051253e-02, 3.32301601e-02, 9.51864715e-01],\n",
       "       [2.81540356e-03, 9.70286186e-03, 9.87481735e-01],\n",
       "       [4.26432973e-03, 1.16331121e-02, 9.84102558e-01],\n",
       "       [5.34181833e-03, 1.38637289e-02, 9.80794453e-01],\n",
       "       [2.52251420e-02, 5.30930476e-02, 9.21681810e-01],\n",
       "       [4.74682674e-03, 1.26274437e-02, 9.82625730e-01],\n",
       "       [2.15800691e-02, 4.61659565e-02, 9.32253974e-01],\n",
       "       [7.97498305e-03, 1.93176990e-02, 9.72707318e-01],\n",
       "       [2.02910095e-02, 4.36951682e-02, 9.36013822e-01],\n",
       "       [4.25096180e-03, 1.16057762e-02, 9.84143262e-01],\n",
       "       [4.60051448e-03, 1.23246873e-02, 9.83074798e-01],\n",
       "       [1.08152210e-02, 2.50937220e-02, 9.64091057e-01],\n",
       "       [5.39994054e-03, 1.39846993e-02, 9.80615360e-01],\n",
       "       [4.39156135e-02, 8.75707072e-02, 8.68513679e-01],\n",
       "       [8.88195627e-02, 1.66045968e-01, 7.45134469e-01],\n",
       "       [5.18552477e-03, 1.35385230e-02, 9.81275952e-01],\n",
       "       [6.83214243e-03, 1.69604900e-02, 9.76207368e-01],\n",
       "       [4.38892632e-03, 1.18885857e-02, 9.83722488e-01],\n",
       "       [3.20200840e-03, 1.23500965e-02, 9.84447895e-01],\n",
       "       [3.55696498e-03, 1.02243267e-02, 9.86218708e-01],\n",
       "       [3.20922345e-02, 6.59397376e-02, 9.01968028e-01],\n",
       "       [3.25583365e-03, 9.67947469e-03, 9.87064692e-01],\n",
       "       [6.52275497e-02, 1.25420414e-01, 8.09352036e-01],\n",
       "       [1.55033454e-02, 3.44051599e-02, 9.50091495e-01]])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Accuracy after last epoch :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of allocation after Epoch 4 : 72.0%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  52 48 50\n"
     ]
    }
   ],
   "source": [
    "accuracy = round((accuracy_score(li, li1)*100),2)\n",
    "print(\"Accuracy of allocation after Epoch {} : {}%\".format(iter+1, accuracy))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li.count(0),li.count(1),li.count(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initially each of the Gaussians consisted of 50 data points. Here we can see that it successfully predicted and classified the total 150 data points in 3 respective Gaussians(0,1,2). \n",
    "* Here I ran my algortithm for 4 epochs since I noticed that after 4th epoch my algorithm started to show no change in parameters which are mean, variance and prior probabilities. Also, when I how many data points were part of each cluster, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different variances :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : priorprob=[0.30379591 0.16129193 0.53491216]\n",
      "Epoch 0 : mean=[-0.41752964  3.32022977  9.28651244]\n",
      "Epoch 0 : var=[ 7.26556547  1.90157733 10.5107976 ]\n",
      "Training Accuracy after Epoch 1 : 76.0%\n",
      "Epoch 1 : priorprob=[0.27202643 0.24936382 0.47860975]\n",
      "Epoch 1 : mean=[-0.52296372  3.70009393  9.60227255]\n",
      "Epoch 1 : var=[ 8.99009478  2.86158074 11.11711744]\n",
      "Training Accuracy after Epoch 2 : 77.33%\n",
      "Epoch 2 : priorprob=[0.27711229 0.28848778 0.43439994]\n",
      "Epoch 2 : mean=[-0.38087645  4.07237943  9.9145151 ]\n",
      "Epoch 2 : var=[ 9.60834796  3.74225517 11.1201174 ]\n",
      "Training Accuracy after Epoch 3 : 85.33%\n",
      "Epoch 3 : priorprob=[0.28845632 0.31073309 0.40081059]\n",
      "Epoch 3 : mean=[-0.22397572  4.39244573 10.16909213]\n",
      "Epoch 3 : var=[ 9.91928415  4.37978983 11.02254412]\n",
      "Training Accuracy after Epoch 4 : 89.33%\n",
      "Epoch 4 : priorprob=[0.30032895 0.32254957 0.37712148]\n",
      "Epoch 4 : mean=[-0.08931848  4.64172266 10.35684882]\n",
      "Epoch 4 : var=[10.10580457  4.7823485  10.91583297]\n",
      "Training Accuracy after Epoch 5 : 92.0%\n",
      "Epoch 5 : priorprob=[0.31057792 0.3284751  0.36094698]\n",
      "Epoch 5 : mean=[ 0.01512184  4.82546131 10.49021224]\n",
      "Epoch 5 : var=[10.22925447  5.0177765  10.81520674]\n",
      "Training Accuracy after Epoch 6 : 94.0%\n",
      "Epoch 6 : priorprob=[0.31864401 0.33139698 0.349959  ]\n",
      "Epoch 6 : mean=[ 0.09167562  4.95725452 10.58443922]\n",
      "Epoch 6 : var=[10.31192985  5.14804772 10.72507354]\n",
      "Training Accuracy after Epoch 7 : 96.67%\n",
      "Epoch 7 : priorprob=[0.32469278 0.33285539 0.34245183]\n",
      "Epoch 7 : mean=[ 0.14586717  5.05066087 10.65156869]\n",
      "Epoch 7 : var=[10.36454448  5.21453249 10.64634785]\n",
      "Training Accuracy after Epoch 8 : 98.67%\n",
      "Epoch 8 : priorprob=[0.32911691 0.33360453 0.33727855]\n",
      "Epoch 8 : mean=[ 0.18328717  5.11665916 10.70002002]\n",
      "Epoch 8 : var=[10.39452501  5.24251216 10.57821177]\n",
      "Training Accuracy after Epoch 9 : 99.33%\n",
      "Epoch 9 : priorprob=[0.33231536 0.33400415 0.33368049]\n",
      "Epoch 9 : mean=[ 0.20857189  5.16343348 10.73551228]\n",
      "Epoch 9 : var=[10.4078048   5.24722343 10.51931697]\n",
      "Training Accuracy after Epoch 10 : 100.0%\n"
     ]
    }
   ],
   "source": [
    "X1=createdata(3)\n",
    "means,var,wprior,posterior=initialize(X1,K)\n",
    "wprior,means,var,accuracy2,li2=GMM(X1,3,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : priorprob=[0.32099957 0.14138499 0.53761544]\n",
      "Epoch 0 : mean=[-1.54752871  3.31178119 10.51948588]\n",
      "Epoch 0 : var=[13.98669165  1.8805     18.07563298]\n",
      "Training Accuracy after Epoch 1 : 72.0%\n",
      "Epoch 1 : priorprob=[0.28882951 0.22971814 0.48145235]\n",
      "Epoch 1 : mean=[-1.54117813  3.5851093  10.90137209]\n",
      "Epoch 1 : var=[18.62759833  3.00261374 19.44275524]\n",
      "Training Accuracy after Epoch 2 : 76.67%\n",
      "Epoch 2 : priorprob=[0.29346914 0.26741311 0.43911776]\n",
      "Epoch 2 : mean=[-1.24711314  3.90304076 11.27074235]\n",
      "Epoch 2 : var=[21.06948723  4.31769766 19.66699698]\n",
      "Training Accuracy after Epoch 3 : 81.33%\n",
      "Epoch 3 : priorprob=[0.30189226 0.29336717 0.40474057]\n",
      "Epoch 3 : mean=[-0.98427124  4.18930851 11.6001626 ]\n",
      "Epoch 3 : var=[22.55781724  5.51393345 19.6492959 ]\n",
      "Training Accuracy after Epoch 4 : 82.0%\n",
      "Epoch 4 : priorprob=[0.31056034 0.30897623 0.38046343]\n",
      "Epoch 4 : mean=[-0.77434744  4.40934265 11.84086901]\n",
      "Epoch 4 : var=[23.46444577  6.49525926 19.59273688]\n",
      "Training Accuracy after Epoch 5 : 86.0%\n",
      "Epoch 5 : priorprob=[0.31776725 0.31816913 0.36406362]\n",
      "Epoch 5 : mean=[-0.61627444  4.56793028 12.00168131]\n",
      "Epoch 5 : var=[24.04442665  7.28398954 19.5777663 ]\n",
      "Training Accuracy after Epoch 6 : 85.33%\n",
      "Epoch 6 : priorprob=[0.32310301 0.3238009  0.35309609]\n",
      "Epoch 6 : mean=[-0.50026642  4.67856881 12.10330836]\n",
      "Epoch 6 : var=[24.44525434  7.92112515 19.62145536]\n",
      "Training Accuracy after Epoch 7 : 84.67%\n",
      "Epoch 7 : priorprob=[0.32681162 0.32739587 0.34579251]\n",
      "Epoch 7 : mean=[-0.41489415  4.75395745 12.16360728]\n",
      "Epoch 7 : var=[24.74682695  8.44121743 19.71502702]\n",
      "Training Accuracy after Epoch 8 : 84.67%\n",
      "Epoch 8 : priorprob=[0.3292939  0.32975178 0.34095432]\n",
      "Epoch 8 : mean=[-0.35069326  4.80410456 12.19587796]\n",
      "Epoch 8 : var=[24.99285638  8.8705707  19.84331082]\n",
      "Training Accuracy after Epoch 9 : 84.67%\n",
      "Epoch 9 : priorprob=[0.3309115  0.33131608 0.33777242]\n",
      "Epoch 9 : mean=[-0.30080274  4.8364267  12.20961527]\n",
      "Epoch 9 : var=[25.20748584  9.22917207 19.99206711]\n",
      "Training Accuracy after Epoch 10 : 84.67%\n"
     ]
    }
   ],
   "source": [
    "X2=createdata(5)\n",
    "means,var,wprior,posterior=initialize(X2,K)\n",
    "wprior,means,var,accuracy3,li3=GMM(X2,3,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 4 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : priorprob=[0.38434615 0.08659788 0.52905597]\n",
      "Epoch 0 : mean=[-4.6580071   3.11694109 14.69448081]\n",
      "Epoch 0 : var=[37.54829617  2.02034978 48.59829648]\n",
      "Training Accuracy after Epoch 1 : 56.67%\n",
      "Epoch 1 : priorprob=[0.3477011  0.17256861 0.47973029]\n",
      "Epoch 1 : mean=[-4.67859324  3.1957804  15.27753612]\n",
      "Epoch 1 : var=[48.64916174  3.46394392 51.86050917]\n",
      "Training Accuracy after Epoch 2 : 59.33%\n",
      "Epoch 2 : priorprob=[0.34473224 0.2027475  0.45252027]\n",
      "Epoch 2 : mean=[-4.41569306  3.30310175 15.703987  ]\n",
      "Epoch 2 : var=[54.56295141  5.66824926 51.36808334]\n",
      "Training Accuracy after Epoch 3 : 61.33%\n",
      "Epoch 3 : priorprob=[0.33710872 0.23663704 0.42625424]\n",
      "Epoch 3 : mean=[-4.297512    3.41666938 16.17357413]\n",
      "Epoch 3 : var=[58.94737614  8.88535254 49.90441664]\n",
      "Training Accuracy after Epoch 4 : 62.0%\n",
      "Epoch 4 : priorprob=[0.32721513 0.26927694 0.40350793]\n",
      "Epoch 4 : mean=[-4.23556038  3.4781826  16.61226677]\n",
      "Epoch 4 : var=[62.48603288 12.46709236 48.20153823]\n",
      "Training Accuracy after Epoch 5 : 62.0%\n",
      "Epoch 5 : priorprob=[0.3208326  0.29202534 0.38714205]\n",
      "Epoch 5 : mean=[-4.11307502  3.48808883 16.93134194]\n",
      "Epoch 5 : var=[65.5866755  15.70892363 46.86534875]\n",
      "Training Accuracy after Epoch 6 : 65.33%\n",
      "Epoch 6 : priorprob=[0.31855561 0.30591    0.37553438]\n",
      "Epoch 6 : mean=[-3.92298925  3.48650468 17.14082688]\n",
      "Epoch 6 : var=[68.46999583 18.45020614 46.13272672]\n",
      "Training Accuracy after Epoch 7 : 66.67%\n",
      "Epoch 7 : priorprob=[0.31888495 0.31424555 0.3668695 ]\n",
      "Epoch 7 : mean=[-3.70033192  3.48549034 17.27730577]\n",
      "Epoch 7 : var=[71.15427535 20.73711325 45.88294674]\n",
      "Training Accuracy after Epoch 8 : 68.0%\n",
      "Epoch 8 : priorprob=[0.32041824 0.31940183 0.36017994]\n",
      "Epoch 8 : mean=[-3.47510948  3.48254135 17.36630494]\n",
      "Epoch 8 : var=[73.61157645 22.65523403 45.93288287]\n",
      "Training Accuracy after Epoch 9 : 68.67%\n",
      "Epoch 9 : priorprob=[0.32227704 0.32277042 0.35495254]\n",
      "Epoch 9 : mean=[-3.2637792   3.47402308 17.42307723]\n",
      "Epoch 9 : var=[75.83115365 24.27901342 46.15204845]\n",
      "Training Accuracy after Epoch 10 : 70.0%\n"
     ]
    }
   ],
   "source": [
    "X2=createdata(10)\n",
    "means,var,wprior,posterior=initialize(X2,K)\n",
    "wprior,means,var,accuracy4,li4=GMM(X2,3,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see a decrease in accuracies when we ran our algorithm for less number of epochs. In our algorithm, we stopped when our posterior probabilities showed almost no change. After trying different values for variance, we noticed that as variance increases, our GMM algorithm takes many iterations to converge. GMM is much more flexible allowing us to generate much better fitting distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
