{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#from random import normal\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score,r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating gaussian distributions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdata(v):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    dis1=np.random.normal(0,v,50)\n",
    "    dis2=np.random.normal(5,v,50)\n",
    "    dis3=np.random.normal(10,v,50)\n",
    "    A=np.concatenate([dis1,dis2])\n",
    "    X=np.concatenate([A,dis3])\n",
    "    X\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=createdata(1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaish\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_hist(data):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    for x in data:\n",
    "        plt.hist(x, bins = 80, normed = True, alpha = 0.7)\n",
    "    #plt.xlim(-10, 20)\n",
    "    plt.figure(figsize=(15,7))\n",
    "    \n",
    "plot_hist(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to initialize random means, variances and prior probabilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X,K):\n",
    "    \n",
    "    m=np.array([2.,3.,7.])             #random means\n",
    "    #m=np.random.choice(X,size=3)      \n",
    "    v=np.ones((3))                     #random variances set to 1 initially\n",
    "    w=np.ones((K))/K                   #random prior weights set to equal\n",
    "    #posterior probability that each input belongs to each of the 3 gaussians\n",
    "    posterior=np.zeros((len(X),K),dtype=float)\n",
    "    return m,v,w,posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True assignment of data samples to respective Gaussians and means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[0]*50 + [1]*50 + [2]*50\n",
    "Y\n",
    "M=[0,5,10]\n",
    "V=[1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,true,K):\n",
    "    diff=[]\n",
    "    summ=0    \n",
    "    for j in range(K):\n",
    "        #d=abs(pred[j]-true[j])/true[j]\n",
    "        diff.append((abs(pred[j]-true[j])))\n",
    "    summ=sum(diff)\n",
    "    accu=round(((1-summ)*100),2)    \n",
    "    #accu=summ\n",
    "        \n",
    "    return summ,accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to allocate the probability of each of the data points belonging to each one of the clusters(Soft Allocation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM(X,K,epochs):\n",
    "    N=len(X)\n",
    "    #K=3\n",
    "    pi=math.pi\n",
    "    iterations=epochs\n",
    "    M=[0.,5.,10.]\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        \n",
    "        #Expection Step : Calculating Posterior probabilities for \n",
    "        for i in range(len(X)):\n",
    "            for k in range(K):\n",
    "\n",
    "                part1=1/math.sqrt(2*pi*(var[k]))\n",
    "                part2=(-1/2)*((X[i]-means[k])**2)\n",
    "                part3= part2/(var[k])\n",
    "                part4=np.exp(part3)\n",
    "                posterior[i][k]=part1 * part4\n",
    "\n",
    "                #posterior[i][k]=wprior[k]*(1/math.sqrt(2*var[k]*pi))*(math.exp(-(1/2)*((X[i]-means[k])**2)))\n",
    "\n",
    "            posterior[i]=posterior[i]/np.sum(posterior[i])\n",
    "        \n",
    "        \n",
    "        #Maximization Step : Updating parameters : mean, variance, and prior probabilities :\n",
    "        meanspred=[]\n",
    "        varpred=[]\n",
    "        diffmean=[]\n",
    "        lossmean=[]\n",
    "        for i in range(K):\n",
    "            wprior[i]=np.sum(posterior[:,i])/N\n",
    "            means[i]=np.dot(posterior[:,i],X[:].T)/(np.sum(posterior[:,i]))\n",
    "            var[i]=np.dot(posterior[:,i],((X-means[i])**2).T)/(np.sum(posterior[:,i]))\n",
    "            meanspred.append(means[i])\n",
    "            varpred.append(var[i])\n",
    "        \n",
    "        ##### Retrieving Predicted class labels from posterior probabilities :\n",
    "        li=[]\n",
    "        for i in range(len(X)):\n",
    "            li.append(np.argmax(posterior[i]))\n",
    "        \n",
    "        \n",
    "        ######Accuracy Calculation for Mean and Variance at each iteration :\n",
    "        meanloss,accumean=accuracy(meanspred,M,K)\n",
    "        varloss,accuvar=accuracy(varpred,V,K)\n",
    "        ######\n",
    "        \n",
    "        \n",
    "        print(\"\\n\")    \n",
    "        print(\"Epoch \" + str(iter+1) + \" : \" + \"prior probability =\"+ str(wprior))\n",
    "        print(\"Epoch \" + str(iter+1) + \" : \" + \"mean =\"+ str(means))\n",
    "        print(\"Epoch \" + str(iter+1) + \" : \"+ \"variance =\" + str(var))\n",
    "        \n",
    "        \n",
    "        #print(meanspred)\n",
    "        print(\"Total mean loss is=> \", meanloss)\n",
    "        #print(\"R2 score for Mean => \".format(round(r2_score(meanspred,M)*100)))\n",
    "        print(\"R2 score of Means after Epoch {} : {}\".format(iter+1,round((r2_score(meanspred,M)),2)))\n",
    "        print(\"Total variance loss is=> \", varloss)\n",
    "        print(\"R2 score of Variance after Epoch {} : {}\".format(iter+1,round((r2_score(varpred,V)),2)))\n",
    "        #print(\"Accuracy of variance after Epoch {} : {}%\".format(iter+1, accuvar))\n",
    "        totalaccuracy = round((accuracy_score(li, Y)*100),2)\n",
    "        print(\"Accuracy of allocation after Epoch {} : {}%\".format(iter+1, totalaccuracy))\n",
    "        #totalaccuracy\n",
    "        \n",
    "        \n",
    "    return (wprior, means,var,totalaccuracy,li)\n",
    "    #means.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 : prior probability =[0.31686045 0.18882409 0.49431546]\n",
      "Epoch 1 : mean =[0.33717436 3.66738246 8.75161024]\n",
      "Epoch 1 : variance =[2.35235318 2.39087316 5.5254408 ]\n",
      "Total mean loss is=>  0.19454544367706067\n",
      "R2 score of Means after Epoch 1 : 0.9\n",
      "Total variance loss is=>  0.4845778092646084\n",
      "R2 score of Variance after Epoch 1 : -2.66\n",
      "Accuracy of allocation after Epoch 1 : 82.0%\n",
      "\n",
      "\n",
      "Epoch 2 : prior probability =[0.29596336 0.28383699 0.42019965]\n",
      "Epoch 2 : mean =[0.07440818 4.23741447 9.28279348]\n",
      "Epoch 2 : variance =[1.55502693 2.29544849 4.62908539]\n",
      "Total mean loss is=>  0.10361334871334661\n",
      "R2 score of Means after Epoch 2 : 0.97\n",
      "Total variance loss is=>  0.36530405414035855\n",
      "R2 score of Variance after Epoch 2 : -1.94\n",
      "Accuracy of allocation after Epoch 2 : 94.0%\n",
      "\n",
      "\n",
      "Epoch 3 : prior probability =[0.30453275 0.31415929 0.38130796]\n",
      "Epoch 3 : mean =[0.01434501 4.55744772 9.67525195]\n",
      "Epoch 3 : variance =[1.18304609 1.63451487 3.33138189]\n",
      "Total mean loss is=>  0.0521096889002393\n",
      "R2 score of Means after Epoch 3 : 0.99\n",
      "Total variance loss is=>  0.20992952297247094\n",
      "R2 score of Variance after Epoch 3 : -1.29\n",
      "Accuracy of allocation after Epoch 3 : 97.33%\n",
      "\n",
      "\n",
      "Epoch 4 : prior probability =[0.31585554 0.32813163 0.35601283]\n",
      "Epoch 4 : mean =[0.04607395 4.74156401 9.98552153]\n",
      "Epoch 4 : variance =[1.15981032 1.13903816 2.09969777]\n",
      "Total mean loss is=>  0.02126589405281533\n",
      "R2 score of Means after Epoch 4 : 1.0\n",
      "Total variance loss is=>  0.09323641669785696\n",
      "R2 score of Variance after Epoch 4 : -1.08\n",
      "Accuracy of allocation after Epoch 4 : 98.0%\n"
     ]
    }
   ],
   "source": [
    "means,var,wprior,posterior=initialize(X,3)\n",
    "#means,var,wprior,posterior.shape\n",
    "wprior,means,var,totalaccuracy,li=GMM(X,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Accuracy after last epoch :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy of allocation : 98.0%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  48 51 51\n"
     ]
    }
   ],
   "source": [
    "finalaccuracy = round((accuracy_score(li, Y)*100),2)\n",
    "print(\"Final Accuracy of allocation : {}%\".format(finalaccuracy))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li.count(0),li.count(1),li.count(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initially each of the Gaussians consisted of 50 data points. Here we can see that it successfully predicted and classified the total 150 data points in 3 respective Gaussians(0,1,2). \n",
    "* Here I ran my algortithm for 4 epochs since I noticed that after 4th epoch my algorithm started to show no change in parameters which are mean, variance and prior probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different variances :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For variance 4, that is S.D = 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 : prior probability =[0.30648201 0.19277336 0.50074463]\n",
      "Epoch 1 : mean =[0.11791059 3.38532905 9.1104891 ]\n",
      "Epoch 1 : variance =[4.84651547 1.70415693 7.15811978]\n",
      "Total mean loss is=>  0.1748061620560036\n",
      "R2 score of Means after Epoch 1 : 0.92\n",
      "Total variance loss is=>  0.713919478115025\n",
      "R2 score of Variance after Epoch 1 : -2.55\n",
      "Accuracy of allocation after Epoch 1 : 74.0%\n",
      "\n",
      "\n",
      "Epoch 2 : prior probability =[0.27949135 0.262496   0.45801265]\n",
      "Epoch 2 : mean =[-0.07774198  3.70496959  9.38829045]\n",
      "Epoch 2 : variance =[5.14716466 2.06819903 7.02706173]\n",
      "Total mean loss is=>  0.13229879590704263\n",
      "R2 score of Means after Epoch 2 : 0.95\n",
      "Total variance loss is=>  0.7494950276779294\n",
      "R2 score of Variance after Epoch 2 : -3.36\n",
      "Accuracy of allocation after Epoch 2 : 78.0%\n",
      "\n",
      "\n",
      "Epoch 3 : prior probability =[0.28189617 0.28496583 0.43313799]\n",
      "Epoch 3 : mean =[-0.08778375  3.94299916  9.58561285]\n",
      "Epoch 3 : variance =[4.95077873 2.35879901 6.63827745]\n",
      "Total mean loss is=>  0.10394478235872709\n",
      "R2 score of Means after Epoch 3 : 0.97\n",
      "Total variance loss is=>  0.7298570122824022\n",
      "R2 score of Variance after Epoch 3 : -4.3\n",
      "Accuracy of allocation after Epoch 3 : 82.67%\n",
      "\n",
      "\n",
      "Epoch 4 : prior probability =[0.28637676 0.29940233 0.4142209 ]\n",
      "Epoch 4 : mean =[-0.07234244  4.13300788  9.738891  ]\n",
      "Epoch 4 : variance =[4.81886978 2.58915933 6.31521612]\n",
      "Total mean loss is=>  0.08002957068527798\n",
      "R2 score of Means after Epoch 4 : 0.98\n",
      "Total variance loss is=>  0.7148830153361876\n",
      "R2 score of Variance after Epoch 4 : -5.45\n",
      "Accuracy of allocation after Epoch 4 : 83.33%\n",
      "\n",
      "\n",
      "Epoch 5 : prior probability =[0.29104564 0.3097749  0.39917946]\n",
      "Epoch 5 : mean =[-0.04366542  4.2846698   9.86070966]\n",
      "Epoch 5 : variance =[4.7785143  2.77776066 6.06667769]\n",
      "Total mean loss is=>  0.059885730844189904\n",
      "R2 score of Means after Epoch 5 : 0.99\n",
      "Total variance loss is=>  0.7081968437820426\n",
      "R2 score of Variance after Epoch 5 : -6.85\n",
      "Accuracy of allocation after Epoch 5 : 84.67%\n",
      "\n",
      "\n",
      "Epoch 6 : prior probability =[0.29570514 0.31715131 0.38714355]\n",
      "Epoch 6 : mean =[-6.32627676e-03  4.40583695e+00  9.95837587e+00]\n",
      "Epoch 6 : variance =[4.80212426 2.93650085 5.87299456]\n",
      "Total mean loss is=>  0.04280756390720047\n",
      "R2 score of Means after Epoch 6 : 0.99\n",
      "Total variance loss is=>  0.7074413118912429\n",
      "R2 score of Variance after Epoch 6 : -8.5\n",
      "Accuracy of allocation after Epoch 6 : 85.33%\n",
      "\n",
      "\n",
      "Epoch 7 : prior probability =[0.30022285 0.32231802 0.37745912]\n",
      "Epoch 7 : mean =[ 0.03523936  4.50338917 10.03728315]\n",
      "Epoch 7 : variance =[4.86315384 3.07241989 5.71916113]\n",
      "Total mean loss is=>  0.03794222281339034\n",
      "R2 score of Means after Epoch 7 : 1.0\n",
      "Total variance loss is=>  0.7103156568152481\n",
      "R2 score of Variance after Epoch 7 : -10.37\n",
      "Accuracy of allocation after Epoch 7 : 86.0%\n",
      "\n",
      "\n",
      "Epoch 8 : prior probability =[0.30447777 0.32591855 0.36960368]\n",
      "Epoch 8 : mean =[ 0.077508    4.58272397 10.10155803]\n",
      "Epoch 8 : variance =[4.94278839 3.18997918 5.59520718]\n",
      "Total mean loss is=>  0.039756137271758196\n",
      "R2 score of Means after Epoch 8 : 1.0\n",
      "Total variance loss is=>  0.7151983164516859\n",
      "R2 score of Variance after Epoch 8 : -12.4\n",
      "Accuracy of allocation after Epoch 8 : 85.33%\n",
      "\n",
      "\n",
      "Epoch 9 : prior probability =[0.30838327 0.32844114 0.36317559]\n",
      "Epoch 9 : mean =[ 0.11812327  4.64782283 10.1543273 ]\n",
      "Epoch 9 : variance =[5.02899299 3.29230546 5.4942921 ]\n",
      "Total mean loss is=>  0.04164184892953472\n",
      "R2 score of Means after Epoch 9 : 1.0\n",
      "Total variance loss is=>  0.7210393701821641\n",
      "R2 score of Variance after Epoch 9 : -14.47\n",
      "Accuracy of allocation after Epoch 9 : 86.0%\n",
      "\n",
      "\n",
      "Epoch 10 : prior probability =[0.31189239 0.33023385 0.35787376]\n",
      "Epoch 10 : mean =[ 0.15571943  4.70158867 10.19794187]\n",
      "Epoch 10 : variance =[5.11455551 3.38173998 5.41151459]\n",
      "Total mean loss is=>  0.04347150862004693\n",
      "R2 score of Means after Epoch 10 : 1.0\n",
      "Total variance loss is=>  0.7271873385700045\n",
      "R2 score of Variance after Epoch 10 : -16.5\n",
      "Accuracy of allocation after Epoch 10 : 86.67%\n",
      "Final Accuracy of allocation : 86.67%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  43 52 55\n"
     ]
    }
   ],
   "source": [
    "X2=createdata(2)\n",
    "means,var,wprior,posterior=initialize(X2,3)\n",
    "wprior,means,var,totalaccuracy2,li2=GMM(X2,3,10)\n",
    "finalaccuracy2 = round((accuracy_score(li2, Y)*100),2)\n",
    "print(\"Final Accuracy of allocation : {}%\".format(finalaccuracy2))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li2.count(0),li2.count(1),li2.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For variance 6.25, that is S.D = 2.5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 : prior probability =[0.30394701 0.17849251 0.51756048]\n",
      "Epoch 1 : mean =[-0.13059147  3.3530637   9.18666587]\n",
      "Epoch 1 : variance =[6.03227425 1.76626288 8.75574925]\n",
      "Total mean loss is=>  0.1727241263729855\n",
      "R2 score of means after Epoch 1 : 0.92\n",
      "Total variance loss is=>  0.9036190918705664\n",
      "R2 score of means after Epoch 1 : -2.47\n",
      "Accuracy of allocation after Epoch 1 : 70.0%\n",
      "\n",
      "\n",
      "Epoch 2 : prior probability =[0.27481004 0.25486875 0.4703212 ]\n",
      "Epoch 2 : mean =[-0.27859314  3.68282801  9.46455624]\n",
      "Epoch 2 : variance =[6.99052385 2.40775107 8.99076705]\n",
      "Total mean loss is=>  0.14208059305441048\n",
      "R2 score of means after Epoch 2 : 0.96\n",
      "Total variance loss is=>  1.0259361311967683\n",
      "R2 score of means after Epoch 2 : -3.47\n",
      "Accuracy of allocation after Epoch 2 : 74.0%\n",
      "\n",
      "\n",
      "Epoch 3 : prior probability =[0.27841973 0.2843063  0.43727397]\n",
      "Epoch 3 : mean =[-0.21468993  3.97557914  9.70318628]\n",
      "Epoch 3 : variance =[7.16601096 3.01721626 8.78699103]\n",
      "Total mean loss is=>  0.10239496751199714\n",
      "R2 score of means after Epoch 3 : 0.98\n",
      "Total variance loss is=>  1.0646812164166073\n",
      "R2 score of means after Epoch 3 : -4.8\n",
      "Accuracy of allocation after Epoch 3 : 78.67%\n",
      "\n",
      "\n",
      "Epoch 4 : prior probability =[0.2860722  0.30347011 0.41045769]\n",
      "Epoch 4 : mean =[-0.13026972  4.23550171  9.90449809]\n",
      "Epoch 4 : variance =[7.24257221 3.51981288 8.56913455]\n",
      "Total mean loss is=>  0.06601799478479496\n",
      "R2 score of means after Epoch 4 : 0.99\n",
      "Total variance loss is=>  1.0887679762517535\n",
      "R2 score of means after Epoch 4 : -6.49\n",
      "Accuracy of allocation after Epoch 4 : 79.33%\n",
      "\n",
      "\n",
      "Epoch 5 : prior probability =[0.29453326 0.31617247 0.38929427]\n",
      "Epoch 5 : mean =[-0.04488301  4.45296211 10.06635556]\n",
      "Epoch 5 : variance =[7.31978527 3.89479363 8.39142693]\n",
      "Total mean loss is=>  0.04388509719633646\n",
      "R2 score of means after Epoch 5 : 0.99\n",
      "Total variance loss is=>  1.1070670553105026\n",
      "R2 score of means after Epoch 5 : -8.33\n",
      "Accuracy of allocation after Epoch 5 : 79.33%\n",
      "\n",
      "\n",
      "Epoch 6 : prior probability =[0.30271263 0.32407195 0.37321542]\n",
      "Epoch 6 : mean =[ 0.03486217  4.62559706 10.19218232]\n",
      "Epoch 6 : variance =[7.40943863 4.15659746 8.24653764]\n",
      "Total mean loss is=>  0.04009649580766588\n",
      "R2 score of means after Epoch 6 : 1.0\n",
      "Total variance loss is=>  1.120838249041513\n",
      "R2 score of means after Epoch 6 : -10.09\n",
      "Accuracy of allocation after Epoch 6 : 78.0%\n",
      "\n",
      "\n",
      "Epoch 7 : prior probability =[0.31000432 0.32873912 0.36125656]\n",
      "Epoch 7 : mean =[ 0.10503069  4.75790445 10.28850451]\n",
      "Epoch 7 : variance =[7.50450414 4.33391534 8.12477703]\n",
      "Total mean loss is=>  0.04237538336104942\n",
      "R2 score of means after Epoch 7 : 1.0\n",
      "Total variance loss is=>  1.1308797676707678\n",
      "R2 score of means after Epoch 7 : -11.6\n",
      "Accuracy of allocation after Epoch 7 : 77.33%\n",
      "\n",
      "\n",
      "Epoch 8 : prior probability =[0.31615036 0.33143586 0.35241379]\n",
      "Epoch 8 : mean =[ 0.16407667  4.85717709 10.36209039]\n",
      "Epoch 8 : variance =[7.59644688 4.45334111 8.02091583]\n",
      "Total mean loss is=>  0.04459933125698589\n",
      "R2 score of means after Epoch 8 : 1.0\n",
      "Total variance loss is=>  1.1380469213592141\n",
      "R2 score of means after Epoch 8 : -12.79\n",
      "Accuracy of allocation after Epoch 8 : 77.33%\n",
      "\n",
      "\n",
      "Epoch 9 : prior probability =[0.32113768 0.33300256 0.34585976]\n",
      "Epoch 9 : mean =[ 0.2122198   4.93076395 10.41852984]\n",
      "Epoch 9 : variance =[7.67967671 4.53434613 7.9322196 ]\n",
      "Total mean loss is=>  0.04666571248591673\n",
      "R2 score of means after Epoch 9 : 1.0\n",
      "Total variance loss is=>  1.1430828297692457\n",
      "R2 score of means after Epoch 9 : -13.67\n",
      "Accuracy of allocation after Epoch 9 : 78.67%\n",
      "\n",
      "\n",
      "Epoch 10 : prior probability =[0.32508415 0.33393962 0.34097623]\n",
      "Epoch 10 : mean =[ 0.25063571  4.98495304 10.46204322]\n",
      "Epoch 10 : variance =[7.75168922 4.58992095 7.85690477]\n",
      "Total mean loss is=>  0.04851505918383782\n",
      "R2 score of means after Epoch 10 : 0.99\n",
      "Total variance loss is=>  1.1465676631997694\n",
      "R2 score of means after Epoch 10 : -14.3\n",
      "Accuracy of allocation after Epoch 10 : 78.67%\n",
      "Final Accuracy of allocation : 78.67%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  46 54 50\n"
     ]
    }
   ],
   "source": [
    "X3=createdata(2.5)\n",
    "means,var,wprior,posterior=initialize(X3,3)\n",
    "wprior,means,var,totalaccuracy3,li3=GMM(X3,3,10)\n",
    "finalaccuracy3 = round((accuracy_score(li3, Y)*100),2)\n",
    "print(\"Final Accuracy of allocation : {}%\".format(finalaccuracy3))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li3.count(0),li3.count(1),li3.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration 4 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For variance 9, that is S.D = 3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1 : prior probability =[0.30379591 0.16129193 0.53491216]\n",
      "Epoch 1 : mean =[-0.41752964  3.32022977  9.28651244]\n",
      "Epoch 1 : variance =[ 7.26556547  1.90157733 10.5107976 ]\n",
      "Total mean loss is=>  0.1873858283991287\n",
      "R2 score of means after Epoch 1 : 0.93\n",
      "Total variance loss is=>  1.1118626931951328\n",
      "R2 score of means after Epoch 1 : -2.45\n",
      "Accuracy of allocation after Epoch 1 : 66.0%\n",
      "\n",
      "\n",
      "Epoch 2 : prior probability =[0.27202643 0.24936382 0.47860975]\n",
      "Epoch 2 : mean =[-0.52296372  3.70009393  9.60227255]\n",
      "Epoch 2 : variance =[ 8.99009478  2.86158074 11.11711744]\n",
      "Total mean loss is=>  0.14803981610184452\n",
      "R2 score of means after Epoch 2 : 0.96\n",
      "Total variance loss is=>  1.331252864313713\n",
      "R2 score of means after Epoch 2 : -3.62\n",
      "Accuracy of allocation after Epoch 2 : 72.67%\n",
      "\n",
      "\n",
      "Epoch 3 : prior probability =[0.27711229 0.28848778 0.43439994]\n",
      "Epoch 3 : mean =[-0.38087645  4.07237943  9.9145151 ]\n",
      "Epoch 3 : variance =[ 9.60834796  3.74225517 11.1201174 ]\n",
      "Total mean loss is=>  0.09293212794352215\n",
      "R2 score of means after Epoch 3 : 0.98\n",
      "Total variance loss is=>  1.4313813690599544\n",
      "R2 score of means after Epoch 3 : -5.06\n",
      "Accuracy of allocation after Epoch 3 : 73.33%\n",
      "\n",
      "\n",
      "Epoch 4 : prior probability =[0.28845632 0.31073309 0.40081059]\n",
      "Epoch 4 : mean =[-0.22397572  4.39244573 10.16909213]\n",
      "Epoch 4 : variance =[ 9.91928415  4.37978983 11.02254412]\n",
      "Total mean loss is=>  0.06670814129218083\n",
      "R2 score of means after Epoch 4 : 0.99\n",
      "Total variance loss is=>  1.4881078726218149\n",
      "R2 score of means after Epoch 4 : -6.55\n",
      "Accuracy of allocation after Epoch 4 : 72.67%\n",
      "\n",
      "\n",
      "Epoch 5 : prior probability =[0.30032895 0.32254957 0.37712148]\n",
      "Epoch 5 : mean =[-0.08931848  4.64172266 10.35684882]\n",
      "Epoch 5 : variance =[10.10580457  4.7823485  10.91583297]\n",
      "Total mean loss is=>  0.05362964202894469\n",
      "R2 score of means after Epoch 5 : 1.0\n",
      "Total variance loss is=>  1.5202657360982579\n",
      "R2 score of means after Epoch 5 : -7.81\n",
      "Accuracy of allocation after Epoch 5 : 73.33%\n",
      "\n",
      "\n",
      "Epoch 6 : prior probability =[0.31057792 0.3284751  0.36094698]\n",
      "Epoch 6 : mean =[ 0.01512184  4.82546131 10.49021224]\n",
      "Epoch 6 : variance =[10.22925447  5.0177765  10.81520674]\n",
      "Total mean loss is=>  0.04532485168778268\n",
      "R2 score of means after Epoch 6 : 1.0\n",
      "Total variance loss is=>  1.5374825140944561\n",
      "R2 score of means after Epoch 6 : -8.7\n",
      "Accuracy of allocation after Epoch 6 : 72.67%\n",
      "\n",
      "\n",
      "Epoch 7 : prior probability =[0.31864401 0.33139698 0.349959  ]\n",
      "Epoch 7 : mean =[ 0.09167562  4.95725452 10.58443922]\n",
      "Epoch 7 : variance =[10.31192985  5.14804772 10.72507354]\n",
      "Total mean loss is=>  0.04792402172109812\n",
      "R2 score of means after Epoch 7 : 0.99\n",
      "Total variance loss is=>  1.5456700744093939\n",
      "R2 score of means after Epoch 7 : -9.28\n",
      "Accuracy of allocation after Epoch 7 : 72.67%\n",
      "\n",
      "\n",
      "Epoch 8 : prior probability =[0.32469278 0.33285539 0.34245183]\n",
      "Epoch 8 : mean =[ 0.14586717  5.05066087 10.65156869]\n",
      "Epoch 8 : variance =[10.36454448  5.21453249 10.64634785]\n",
      "Total mean loss is=>  0.05653978193374441\n",
      "R2 score of means after Epoch 8 : 0.99\n",
      "Total variance loss is=>  1.548361654915803\n",
      "R2 score of means after Epoch 8 : -9.61\n",
      "Accuracy of allocation after Epoch 8 : 72.0%\n",
      "\n",
      "\n",
      "Epoch 9 : prior probability =[0.32911691 0.33360453 0.33727855]\n",
      "Epoch 9 : mean =[ 0.18328717  5.11665916 10.70002002]\n",
      "Epoch 9 : variance =[10.39452501  5.24251216 10.57821177]\n",
      "Total mean loss is=>  0.06666442307145433\n",
      "R2 score of means after Epoch 9 : 0.99\n",
      "Total variance loss is=>  1.547683262519747\n",
      "R2 score of means after Epoch 9 : -9.79\n",
      "Accuracy of allocation after Epoch 9 : 71.33%\n",
      "\n",
      "\n",
      "Epoch 10 : prior probability =[0.33231536 0.33400415 0.33368049]\n",
      "Epoch 10 : mean =[ 0.20857189  5.16343348 10.73551228]\n",
      "Epoch 10 : variance =[10.4078048   5.24722343 10.51931697]\n",
      "Total mean loss is=>  0.073834509458838\n",
      "R2 score of means after Epoch 10 : 0.99\n",
      "Total variance loss is=>  1.5449563465185079\n",
      "R2 score of means after Epoch 10 : -9.87\n",
      "Accuracy of allocation after Epoch 10 : 70.67%\n",
      "Final Accuracy of allocation : 70.67%\n",
      "Number of samples classifed for Gaussians 0, 1, 2 :  47 56 47\n"
     ]
    }
   ],
   "source": [
    "X4=createdata(3)\n",
    "means,var,wprior,posterior=initialize(X4,3)\n",
    "wprior,means,var,totalaccuracy4,li4 = GMM(X4,3,10)\n",
    "finalaccuracy4 = round((accuracy_score(li4, Y)*100),2)\n",
    "print(\"Final Accuracy of allocation : {}%\".format(finalaccuracy4))\n",
    "print(\"Number of samples classifed for Gaussians 0, 1, 2 : \",li4.count(0),li4.count(1),li4.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see a decrease in accuracies when we ran our algorithm for less number of epochs. In our algorithm, we stopped when our posterior probabilities showed almost no change. After trying different values for variance, we noticed that as variance increases, our GMM algorithm takes many iterations to converge. GMM is much more flexible allowing us to generate much better fitting distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
